{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7e8016-4f25-40d3-9902-bef1ab99cd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from time import perf_counter\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "# preserve threads for GPU\n",
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "\n",
    "# constrain VRAM usage\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# enable mixed-precision training\n",
    "keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc9c260-c6be-4283-8cbf-d99bd8ce6fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, length, random=True):\n",
    "    if random:\n",
    "        predictions, labels = model.random_generate(length**2)\n",
    "        plt.figure(figsize=(length, length))\n",
    "        for i in range(length**2):\n",
    "            plt.subplot(length, length, i+1)\n",
    "            plt.imshow(predictions[i], cmap='gray')\n",
    "            plt.title(labels[i])\n",
    "            plt.axis('off')\n",
    "    else:\n",
    "        predictions = model.categorical_generate(length)\n",
    "        plt.figure(figsize=(10, length))\n",
    "        for i in range(10*length):\n",
    "            plt.subplot(length, 10, i+1)\n",
    "            plt.imshow(predictions[i], cmap='gray')\n",
    "            plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_history(history):\n",
    "    plt.plot(history['disc_loss'], label='Discriminator Loss')\n",
    "    plt.plot(history['gen_loss'], label='Generator Loss')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc564b-bd68-48ab-9f10-f48e7e4c0c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN:\n",
    "    def __init__(self, disc, gen, disc_opt, gen_opt, z_dim=64):\n",
    "        self.disc = disc\n",
    "        self.gen = gen\n",
    "        self.disc_opt = disc_opt\n",
    "        self.gen_opt = gen_opt\n",
    "        self.z_dim = z_dim\n",
    "        self.BCE_loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self.history = {'disc_loss':[], 'gen_loss':[]}\n",
    "        self.disc_mean_loss = keras.metrics.Mean(name='disc_loss')\n",
    "        self.gen_mean_loss = keras.metrics.Mean(name='gen_loss')\n",
    "    \n",
    "    def random_generate(self, n_images):\n",
    "        noise = tf.random.normal([n_images, self.z_dim])\n",
    "        labels = tf.random.uniform([n_images], minval=0, maxval=10, dtype=tf.int32)\n",
    "        one_hot_labels = tf.one_hot(labels, 10)\n",
    "        noise_and_labels = tf.concat((noise, one_hot_labels), axis=-1)\n",
    "        return (self.gen(noise_and_labels, training=False).numpy().reshape(-1, 28, 28) + 1.) / 2, labels.numpy()\n",
    "    \n",
    "    def categorical_generate(self, n_each):\n",
    "        noise = tf.random.normal([n_each*10, self.z_dim])\n",
    "        labels = tf.tile(tf.range(10), [n_each])\n",
    "        one_hot_labels = tf.one_hot(labels, 10)\n",
    "        noise_and_labels = tf.concat((noise, one_hot_labels), axis=-1)\n",
    "        return (self.gen(noise_and_labels, training=False).numpy().reshape(-1, 28, 28) + 1.) / 2\n",
    "\n",
    "    # execute in graph mode with XLA for superior performance\n",
    "    @tf.function(jit_compile=True)\n",
    "    def train_step(self, real, labels):\n",
    "        batch_size = real.shape[0]\n",
    "        one_hot_labels = tf.one_hot(labels, 10)\n",
    "        tiled_one_hot_labels = tf.tile(tf.reshape(one_hot_labels, [-1, 1, 1, 10]), [1, 28, 28, 1])\n",
    "        noise = tf.random.normal([batch_size, self.z_dim])\n",
    "        noise_and_labels = tf.concat((noise, one_hot_labels), axis=-1)\n",
    "        fake = self.gen(noise_and_labels, training=False)\n",
    "        fake_and_labels = tf.concat((fake, tiled_one_hot_labels), axis=-1)\n",
    "        real_and_labels = tf.concat((real, tiled_one_hot_labels), axis=-1)\n",
    "        concatenated = tf.concat((fake_and_labels, real_and_labels), axis=0)\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            disc_concatenated_pred = self.disc(concatenated)\n",
    "            concatenated_labels = tf.concat((tf.zeros([batch_size, 1]), tf.ones([batch_size, 1])), axis=0)\n",
    "            disc_loss = self.BCE_loss(concatenated_labels, disc_concatenated_pred)\n",
    "        grad_of_disc = disc_tape.gradient(disc_loss, self.disc.trainable_variables)\n",
    "        self.disc_opt.apply_gradients(zip(grad_of_disc, self.disc.trainable_variables))\n",
    "        self.disc_mean_loss.update_state(disc_loss)\n",
    "        noise = tf.random.normal([batch_size, self.z_dim])\n",
    "        noise_and_labels = tf.concat((noise, one_hot_labels), axis=-1)\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            fake = self.gen(noise_and_labels)\n",
    "            fake_and_labels = tf.concat((fake, tiled_one_hot_labels), axis=-1)\n",
    "            disc_fake_pred = self.disc(fake_and_labels, training=False)\n",
    "            gen_loss = self.BCE_loss(tf.ones_like(disc_fake_pred), disc_fake_pred)\n",
    "        grad_of_gen = gen_tape.gradient(gen_loss, self.gen.trainable_variables)\n",
    "        self.gen_opt.apply_gradients(zip(grad_of_gen, self.gen.trainable_variables))\n",
    "        self.gen_mean_loss.update_state(gen_loss)\n",
    "        \n",
    "    def fit(self, data, epochs=1):\n",
    "        for epoch in range(epochs):\n",
    "            tic = perf_counter()\n",
    "            self.disc_mean_loss.reset_states()\n",
    "            self.gen_mean_loss.reset_states()\n",
    "            for real, labels in tqdm(data):\n",
    "                model.train_step(real, labels)\n",
    "            self.history['disc_loss'].append(self.disc_mean_loss.result().numpy())\n",
    "            self.history['gen_loss'].append(self.gen_mean_loss.result().numpy())\n",
    "            display.clear_output(wait=True)\n",
    "            print(\"Epoch %d/%d - %.1fs | disc_loss: %.5f - gen_loss: %.5f\"%(\n",
    "                epoch+1, EPOCHS, perf_counter() - tic, self.history['disc_loss'][-1], self.history['gen_loss'][-1]))\n",
    "            test_model(self, 4, random=False)\n",
    "            show_history(self.history)\n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956d05ed-f9e1-4b88-a862-1ab6fc8b0f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BUFFER = 70000\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "DIMS = (28, 28, 11)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "z_dim = 64\n",
    "lr = 2e-4\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83db0ec3-396a-4fe3-973f-32d62feecfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X = np.concatenate((x_train, x_test)).astype(np.float32).reshape(-1, 28, 28, 1) / 127.5 - 1.\n",
    "Y = np.concatenate((y_train, y_test))\n",
    "dataloader = tf.data.Dataset.from_tensor_slices((X, Y)).shuffle(TRAIN_BUFFER).batch(\n",
    "    BATCH_SIZE, num_parallel_calls=AUTOTUNE, deterministic=False, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "\n",
    "discriminator = keras.Sequential([\n",
    "    layers.InputLayer(DIMS), \n",
    "    layers.Conv2D(32, 4, strides=2, padding='same', use_bias=False),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(alpha=0.2),\n",
    "\n",
    "    layers.Conv2D(64, 4, strides=2, padding='same', use_bias=False),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(alpha=0.2),\n",
    "\n",
    "    layers.Conv2D(128, 3, strides=1, padding='same', use_bias=False),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(alpha=0.2),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1, dtype=tf.float32)\n",
    "])\n",
    "\n",
    "generator = keras.Sequential([\n",
    "    layers.InputLayer([z_dim+10]),\n",
    "    layers.Dense(7*7*128, use_bias=False),\n",
    "    layers.Reshape([7, 7, 128]),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.ReLU(),\n",
    "\n",
    "    layers.Conv2DTranspose(64, 4, strides=2, padding='same', use_bias=False),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.ReLU(),\n",
    "\n",
    "    layers.Conv2DTranspose(32, 4, strides=2, padding='same', use_bias=False),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.ReLU(),\n",
    "\n",
    "    layers.Conv2D(1, 3, strides=1, padding='same', activation='tanh', dtype=tf.float32)\n",
    "])\n",
    "\n",
    "model = CGAN(\n",
    "    gen=generator,\n",
    "    disc=discriminator,\n",
    "    gen_opt=keras.optimizers.Adam(learning_rate=lr, beta_1=beta_1, beta_2=beta_2),\n",
    "    disc_opt=keras.optimizers.Adam(learning_rate=lr, beta_1=beta_1, beta_2=beta_2),\n",
    "    z_dim=z_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac886ff-5c31-45b3-9e41-9e31bfec53aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(dataloader, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab463e4-50de-4c7d-88da-f08fce89ab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, 8, random=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e4200f-d433-4c63-8d02-294ddde2509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = \"./CGAN\"\n",
    "#model.gen.save(model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4357e15a-cde1-46ee-9910-a1e1b72df625",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"model_path = \"./CGAN\"\n",
    "loaded_model = CGAN(\n",
    "    gen=keras.models.load_model(model_path, compile=False),\n",
    "    disc=None,\n",
    "    gen_opt=None,\n",
    "    disc_opt=None,\n",
    "    z_dim=64\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c5d602-8c26-479a-9b61-e5af93f1fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_model(loaded_model, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6b4c1e-19c2-4b3f-83fc-cfd7a090e2d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}