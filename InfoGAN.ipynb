{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7e8016-4f25-40d3-9902-bef1ab99cd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from time import perf_counter\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    \n",
    "keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc9c260-c6be-4283-8cbf-d99bd8ce6fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, length, mode='random', index=4):\n",
    "    if mode == 'random':\n",
    "        predictions = model.random_generate(length**2)\n",
    "        plt.figure(figsize=(length, length))\n",
    "        for i in range(length**2):\n",
    "            plt.subplot(length, length, i+1)\n",
    "            plt.imshow(predictions[i], cmap='gray')\n",
    "            plt.axis('off')\n",
    "    if mode == 'categorical':\n",
    "        predictions = model.categorical_generate(length)\n",
    "        plt.figure(figsize=(10, length))\n",
    "        for i in range(10*length):\n",
    "            plt.subplot(length, 10, i+1)\n",
    "            plt.imshow(predictions[i], cmap='gray')\n",
    "            plt.axis('off')\n",
    "    if mode == 'interpolated':\n",
    "        predictions = model.interpolated_generate(length**2, digit=index)\n",
    "        plt.figure(figsize=(length, length))\n",
    "        for i in range(length**2):\n",
    "            plt.subplot(length, length, i+1)\n",
    "            plt.imshow(predictions[i], cmap='gray')\n",
    "            plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_history(history):\n",
    "    plt.plot(history['disc_loss'], label='Discriminator Loss')\n",
    "    plt.plot(history['gen_loss'], label='Generator Loss')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc564b-bd68-48ab-9f10-f48e7e4c0c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoGAN:\n",
    "    def __init__(self, disc, gen, disc_opt, gen_opt, z_dim=64):\n",
    "        self.disc = disc\n",
    "        self.gen = gen\n",
    "        self.disc_opt = disc_opt\n",
    "        self.gen_opt = gen_opt\n",
    "        self.z_dim = z_dim\n",
    "        self.BCE_loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self.CCE_loss = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        self.MSE_loss = keras.losses.MeanSquaredError()\n",
    "        self.history = {'disc_loss':[], 'gen_loss':[]}\n",
    "        self.disc_mean_loss = keras.metrics.Mean(name='disc_loss')\n",
    "        self.gen_mean_loss = keras.metrics.Mean(name='gen_loss')\n",
    "    \n",
    "    def random_generate(self, length):\n",
    "        noise = tf.random.normal([length**2, self.z_dim])\n",
    "        c_labels = tf.random.uniform([length**2], minval=0, maxval=10, dtype=tf.int32)\n",
    "        c_one_hot_labels = tf.one_hot(c_labels, 10)\n",
    "        c_features = tf.random.normal([batch_size, 2])\n",
    "        noise_and_labels = tf.concat((noise, c_one_hot_labels, c_features), axis=-1)\n",
    "        return (self.gen(noise_and_labels, training=False).numpy().reshape(-1, 28, 28) + 1.) / 2\n",
    "    \n",
    "    def categorical_generate(self, n_each):\n",
    "        noise = tf.random.normal([n_each*10, self.z_dim])\n",
    "        c_labels = tf.tile(tf.range(10), [n_each])\n",
    "        c_one_hot_labels = tf.one_hot(c_labels, 10)\n",
    "        c_features = tf.random.normal([n_each*10, 2])\n",
    "        noise_and_labels = tf.concat((noise, c_one_hot_labels, c_features), axis=-1)\n",
    "        return (self.gen(noise_and_labels, training=False).numpy().reshape(-1, 28, 28) + 1.) / 2\n",
    "    \n",
    "    def interpolated_generate(self, length, digit=4):\n",
    "        noise = tf.tile(tf.random.normal([1, self.z_dim]), [length**2, 1])\n",
    "        c_one_hot_labels = tf.tile(tf.one_hot([digit], 10), [length**2, 1])\n",
    "        inter = tf.cast(tf.expand_dims(tf.linspace(-3, 3, length), axis=-1), tf.float32)\n",
    "        noise_and_labels = tf.concat((noise, c_one_hot_labels, tf.tile(inter, [length, 1]), tf.repeat(inter, length, axis=0)), axis=-1)\n",
    "        return (self.gen(noise_and_labels, training=False).numpy().reshape(-1, 28, 28) + 1.) / 2\n",
    "    \n",
    "    @tf.function(jit_compile=True)\n",
    "    def train_step(self, real):\n",
    "        batch_size = real.shape[0]\n",
    "        noise = tf.random.normal([batch_size, self.z_dim])\n",
    "        c_labels = tf.random.uniform([batch_size], minval=0, maxval=10, dtype=tf.int32)\n",
    "        c_one_hot_labels = tf.one_hot(c_labels, 10)\n",
    "        c_features = tf.random.normal([batch_size, 2])\n",
    "        noise_and_labels = tf.concat((noise, c_one_hot_labels, c_features), axis=-1)\n",
    "        fake = self.gen(noise_and_labels, training=False)\n",
    "        concatenated = tf.concat((fake, real), axis=0)\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            disc_concat_pred, disc_concat_approx = self.disc(concatenated)\n",
    "            concatenated_labels = tf.concat((tf.zeros([batch_size, 1]), tf.ones([batch_size, 1])), axis=0)\n",
    "            disc_loss = self.BCE_loss(concatenated_labels, disc_concat_pred)\n",
    "            category_loss = self.CCE_loss(c_one_hot_labels, disc_concat_approx[:batch_size, :10])\n",
    "            feature_loss = self.MSE_loss(c_features, disc_concat_approx[:batch_size, 10:])\n",
    "            total_disc_loss = disc_loss + category_loss + feature_loss\n",
    "        grad_of_disc = disc_tape.gradient(total_disc_loss, self.disc.trainable_variables)\n",
    "        self.disc_opt.apply_gradients(zip(grad_of_disc, self.disc.trainable_variables))\n",
    "        self.disc_mean_loss.update_state(total_disc_loss)\n",
    "        \n",
    "        noise = tf.random.normal([batch_size, self.z_dim])\n",
    "        c_labels = tf.random.uniform([batch_size], minval=0, maxval=10, dtype=tf.int32)\n",
    "        c_one_hot_labels = tf.one_hot(c_labels, 10)\n",
    "        c_features = tf.random.normal([batch_size, 2])\n",
    "        noise_and_labels = tf.concat((noise, c_one_hot_labels, c_features), axis=-1)\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            fake = self.gen(noise_and_labels)\n",
    "            disc_fake_pred, disc_approx = self.disc(fake, training=False)\n",
    "            gen_loss = self.BCE_loss(tf.ones_like(disc_fake_pred), disc_fake_pred)\n",
    "            category_loss = self.CCE_loss(c_one_hot_labels, disc_approx[:, :10])\n",
    "            feature_loss = self.MSE_loss(c_features, disc_approx[:, 10:])\n",
    "            total_gen_loss = gen_loss + category_loss + feature_loss\n",
    "        grad_of_gen = gen_tape.gradient(total_gen_loss, self.gen.trainable_variables)\n",
    "        self.gen_opt.apply_gradients(zip(grad_of_gen, self.gen.trainable_variables))\n",
    "        self.gen_mean_loss.update_state(total_gen_loss)\n",
    "    \n",
    "    def fit(self, data, epochs=1):\n",
    "        for epoch in range(epochs):\n",
    "            tic = perf_counter()\n",
    "            self.disc_mean_loss.reset_states()\n",
    "            self.gen_mean_loss.reset_states()\n",
    "            for real in tqdm(data):\n",
    "                model.train_step(real)\n",
    "            self.history['disc_loss'].append(self.disc_mean_loss.result().numpy())\n",
    "            self.history['gen_loss'].append(self.gen_mean_loss.result().numpy())\n",
    "            display.clear_output(wait=True)\n",
    "            print(\"Epoch %d/%d - %.1fs | disc_loss: %.5f - gen_loss: %.5f\"%(\n",
    "                epoch+1, EPOCHS, perf_counter() - tic, self.history['disc_loss'][-1], self.history['gen_loss'][-1]))\n",
    "            test_model(self, 4, mode='categorical')\n",
    "            show_history(self.history)\n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9333135-0eb4-47d3-bdff-31f89e82affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BUFFER=70000\n",
    "BATCH_SIZE=64\n",
    "EPOCHS = 100\n",
    "DIMS = (28, 28, 1)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "z_dim = 64\n",
    "c_dim = 12\n",
    "lr = 2e-4\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e1851c-ee3c-40ce-b8ad-76ee58230f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
    "X = np.concatenate((x_train, x_test)).astype(np.float32).reshape(-1, 28, 28, 1) / 127.5 - 1.\n",
    "dataloader = tf.data.Dataset.from_tensor_slices(X).shuffle(TRAIN_BUFFER).batch(\n",
    "    BATCH_SIZE, num_parallel_calls=AUTOTUNE, deterministic=False, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "\n",
    "inputs = keras.Input(DIMS)\n",
    "t = layers.Conv2D(32, 4, strides=2, padding='same', use_bias=False)(inputs)\n",
    "t = layers.BatchNormalization()(t)\n",
    "t = layers.LeakyReLU(alpha=0.2)(t)\n",
    "\n",
    "t = layers.Conv2D(64, 4, strides=2, padding='same', use_bias=False)(t)\n",
    "t = layers.BatchNormalization()(t)\n",
    "t = layers.LeakyReLU(alpha=0.2)(t)\n",
    "\n",
    "t = layers.Conv2D(128, 4, strides=2, padding='same', use_bias=False)(t)\n",
    "t = layers.BatchNormalization()(t)\n",
    "t = layers.LeakyReLU(alpha=0.2)(t)\n",
    "t = layers.Flatten()(t)\n",
    "\n",
    "classification = layers.Dense(1, dtype=tf.float32)(t)\n",
    "approximation = layers.Dense(c_dim, dtype=tf.float32)(t)\n",
    "discriminator = keras.Model(inputs, [classification, approximation])\n",
    "\n",
    "generator = keras.Sequential([\n",
    "    layers.InputLayer([z_dim+c_dim]),\n",
    "    layers.Dense(7*7*128, use_bias=False),\n",
    "    layers.Reshape([7, 7, 128]),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.ReLU(),\n",
    "\n",
    "    layers.Conv2DTranspose(64, 4, strides=2, padding='same', use_bias=False),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.ReLU(),\n",
    "\n",
    "    layers.Conv2DTranspose(32, 4, strides=2, padding='same', use_bias=False),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.ReLU(),\n",
    "\n",
    "    layers.Conv2D(1, 3, strides=1, padding='same', activation='tanh', dtype=tf.float32)\n",
    "])\n",
    "\n",
    "model = InfoGAN(\n",
    "    disc=discriminator,\n",
    "    gen=generator,\n",
    "    disc_opt=keras.optimizers.Adam(learning_rate=lr, beta_1=beta_1, beta_2=beta_2),\n",
    "    gen_opt=keras.optimizers.Adam(learning_rate=lr, beta_1=beta_1, beta_2=beta_2),\n",
    "    z_dim=z_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e33f96-37e8-4a4c-ba53-d047172fe627",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(dataloader, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab463e4-50de-4c7d-88da-f08fce89ab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, 8, mode='interpolated', index=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e4200f-d433-4c63-8d02-294ddde2509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = \"./InfoGAN\"\n",
    "#model.gen.save(model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4357e15a-cde1-46ee-9910-a1e1b72df625",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"model_path = \"./InfoGAN\"\n",
    "loaded_model = InfoGAN(\n",
    "    gen=keras.models.load_model(model_path, compile=False),\n",
    "    disc=None,\n",
    "    gen_opt=None,\n",
    "    disc_opt=None,\n",
    "    z_dim=64\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c5d602-8c26-479a-9b61-e5af93f1fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_model(loaded_model, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6b4c1e-19c2-4b3f-83fc-cfd7a090e2d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
